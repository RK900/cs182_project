{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-vertex",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:47.287725Z",
     "iopub.status.busy": "2021-05-05T19:59:47.286115Z",
     "iopub.status.idle": "2021-05-05T19:59:47.409981Z",
     "shell.execute_reply": "2021-05-05T19:59:47.409394Z"
    },
    "papermill": {
     "duration": 0.14312,
     "end_time": "2021-05-05T19:59:47.410114",
     "exception": false,
     "start_time": "2021-05-05T19:59:47.266994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from segtok import tokenizer\n",
    "from utils import *\n",
    "import hickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-diploma",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:47.443999Z",
     "iopub.status.busy": "2021-05-05T19:59:47.443415Z",
     "iopub.status.idle": "2021-05-05T19:59:47.450380Z",
     "shell.execute_reply": "2021-05-05T19:59:47.449849Z"
    },
    "papermill": {
     "duration": 0.025267,
     "end_time": "2021-05-05T19:59:47.450490",
     "exception": false,
     "start_time": "2021-05-05T19:59:47.425223",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Key Hyperparameters\n",
    "balanced_categories = False\n",
    "enable_orig = \"orig\"\n",
    "enable_aug = False\n",
    "enable_aug3 = False\n",
    "max_training_samples = 200000\n",
    "max_tokenized_length = 64\n",
    "num_sentences = 10\n",
    "valid_percent = 0.01\n",
    "sentence_pairs = False\n",
    "\n",
    "batch_size_finetuning = 32\n",
    "epochs_finetuning = 1\n",
    "lr_finetuning = 1e-5\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "main_model_lr = 1e-5\n",
    "\n",
    "lstm_hidden_size = 1024\n",
    "regressive_style_finetuning = False\n",
    "lstm_bidi = False\n",
    "\n",
    "experiment_id = f\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-mechanism",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:47.481217Z",
     "iopub.status.busy": "2021-05-05T19:59:47.480732Z",
     "iopub.status.idle": "2021-05-05T19:59:47.489621Z",
     "shell.execute_reply": "2021-05-05T19:59:47.489984Z"
    },
    "papermill": {
     "duration": 0.024778,
     "end_time": "2021-05-05T19:59:47.490098",
     "exception": false,
     "start_time": "2021-05-05T19:59:47.465320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "experiment_dir = f\"completed-experiments/{experiment_id}\"\n",
    "if not os.path.exists(experiment_dir):\n",
    "    os.makedirs(experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-prime",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:47.514933Z",
     "iopub.status.busy": "2021-05-05T19:59:47.514641Z",
     "iopub.status.idle": "2021-05-05T19:59:50.118615Z",
     "shell.execute_reply": "2021-05-05T19:59:50.117970Z"
    },
    "papermill": {
     "duration": 2.617011,
     "end_time": "2021-05-05T19:59:50.118741",
     "exception": false,
     "start_time": "2021-05-05T19:59:47.501730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_parsing import *\n",
    "data = load_dataset(\"./yelp_review_training_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-tampa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:50.154869Z",
     "iopub.status.busy": "2021-05-05T19:59:50.154250Z",
     "iopub.status.idle": "2021-05-05T19:59:53.089985Z",
     "shell.execute_reply": "2021-05-05T19:59:53.089435Z"
    },
    "papermill": {
     "duration": 2.955392,
     "end_time": "2021-05-05T19:59:53.090116",
     "exception": false,
     "start_time": "2021-05-05T19:59:50.134724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from training_utils import split_train_validation\n",
    "from text_preprocessing import preprocess\n",
    "import random\n",
    "from math import ceil\n",
    "\n",
    "def get_train_valid():\n",
    "    global data\n",
    "    if enable_orig == \"big\" or enable_orig == \"big-preprocess\":\n",
    "        data += load_gen_dataset(\"./yelp_dataset/more_yelp_234.json\")\n",
    "    if balanced_categories:\n",
    "        orig_data_uniform = split_equally(data, ceil(max_training_samples / 0.99))\n",
    "    else:\n",
    "        orig_data_uniform = data\n",
    "    orig_train_x, valid_x, orig_train_y, valid_y = split_train_validation(orig_data_uniform, 0.01)\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "\n",
    "    if enable_aug:\n",
    "        aug_data = load_gen_dataset(\"./new_data.json\") + load_gen_dataset(\"./new_data2.json\")\n",
    "        train_x += [i[0] for i in aug_data]\n",
    "        train_y += [i[1] for i in aug_data]\n",
    "\n",
    "    if enable_aug3:\n",
    "        aug_data3 = load_gen_dataset(\"./new_data3.json\")\n",
    "        train_x += [i[0] for i in aug_data3]\n",
    "        train_y += [i[1] for i in aug_data3]\n",
    "        \n",
    "    if enable_orig:\n",
    "        train_x += orig_train_x\n",
    "        train_y += orig_train_y\n",
    "    \n",
    "    train_x = train_x[:max_training_samples]\n",
    "    train_y = train_y[:max_training_samples]\n",
    "\n",
    "    if enable_orig == \"preprocess\" or enable_orig == \"big-preprocess\":\n",
    "        train_x = preprocess(train_x)\n",
    "        valid_x = preprocess(valid_x)    \n",
    "\n",
    "    paired_train = list(zip(train_x, train_y))\n",
    "    random.shuffle(paired_train)\n",
    "    train_x = [i[0] for i in paired_train]\n",
    "    train_y = [i[1] for i in paired_train]\n",
    "\n",
    "    return [x.encode(\"utf-8\") for x in train_x], [x.encode(\"utf-8\") for x in valid_x], train_y, valid_y\n",
    "\n",
    "split_key = f\"cache-core/split-data-{valid_percent}-orig-{enable_orig}-aug12-{enable_aug}-aug3-{enable_aug3}-max-{max_training_samples}\"\n",
    "if not balanced_categories:\n",
    "    split_key += \"-unbalanced\"\n",
    "train_x, valid_x, train_y, valid_y = memo_load(\n",
    "    get_train_valid,\n",
    "    split_key\n",
    ")\n",
    "split_hash = hash_file(split_key + \".hkl\")\n",
    "\n",
    "train_x = [x.decode(\"utf-8\") for x in train_x]\n",
    "valid_x = [x.decode(\"utf-8\") for x in valid_x]\n",
    "\n",
    "print(len(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-specific",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:53.122464Z",
     "iopub.status.busy": "2021-05-05T19:59:53.121867Z",
     "iopub.status.idle": "2021-05-05T19:59:53.133179Z",
     "shell.execute_reply": "2021-05-05T19:59:53.132587Z"
    },
    "papermill": {
     "duration": 0.028165,
     "end_time": "2021-05-05T19:59:53.133294",
     "exception": false,
     "start_time": "2021-05-05T19:59:53.105129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(train_x))\n",
    "print(len(train_y))\n",
    "print(len(valid_x))\n",
    "print(len(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-emission",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:53.164349Z",
     "iopub.status.busy": "2021-05-05T19:59:53.163871Z",
     "iopub.status.idle": "2021-05-05T19:59:54.053298Z",
     "shell.execute_reply": "2021-05-05T19:59:54.052694Z"
    },
    "papermill": {
     "duration": 0.904874,
     "end_time": "2021-05-05T19:59:54.053418",
     "exception": false,
     "start_time": "2021-05-05T19:59:53.148544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-silly",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:54.088081Z",
     "iopub.status.busy": "2021-05-05T19:59:54.087495Z",
     "iopub.status.idle": "2021-05-05T19:59:55.261425Z",
     "shell.execute_reply": "2021-05-05T19:59:55.260869Z"
    },
    "papermill": {
     "duration": 1.191923,
     "end_time": "2021-05-05T19:59:55.261554",
     "exception": false,
     "start_time": "2021-05-05T19:59:54.069631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-saver",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:55.305804Z",
     "iopub.status.busy": "2021-05-05T19:59:55.305209Z",
     "iopub.status.idle": "2021-05-05T19:59:55.727013Z",
     "shell.execute_reply": "2021-05-05T19:59:55.727279Z"
    },
    "papermill": {
     "duration": 0.448555,
     "end_time": "2021-05-05T19:59:55.727369",
     "exception": false,
     "start_time": "2021-05-05T19:59:55.278814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fine tune the BERT\n",
    "import numpy as np\n",
    "\n",
    "def get_finetuning_data():\n",
    "    train_x_numerized = []\n",
    "    train_x_mask = []\n",
    "    train_y_per_sentence = []\n",
    "    for i, text in tqdm(list(enumerate(train_x))):\n",
    "        doc = nlp(text)\n",
    "        sents = [str(sent) for sent in doc.sents]\n",
    "        if sentence_pairs == True and len(sents) > 1:\n",
    "            sents = [a + b for a, b in zip(sents, sents[1:])]\n",
    "        if sentence_pairs == \"3\" and len(sents) > 2:\n",
    "            sents = [a + b + c for a, b, c in zip(sents, sents[1:], sents[2:])]\n",
    "        for sentence in sents[:num_sentences]:\n",
    "            tokenized = tokenizer(sentence, truncation=True, padding=\"max_length\", max_length=max_tokenized_length)[0]\n",
    "            train_x_numerized.append(tokenized.ids)\n",
    "            train_x_mask.append(tokenized.attention_mask)\n",
    "            train_y_per_sentence.append(train_y[i])\n",
    "\n",
    "    valid_x_numerized = []\n",
    "    valid_x_mask = []\n",
    "    valid_y_per_sentence = []\n",
    "    for i, text in tqdm(list(enumerate(valid_x))):\n",
    "        doc = nlp(text)\n",
    "        sents = [str(sent) for sent in doc.sents]\n",
    "        if sentence_pairs == True and len(sents) > 1:\n",
    "            sents = [a + b for a, b in zip(sents, sents[1:])]\n",
    "        if sentence_pairs == \"3\" and len(sents) > 2:\n",
    "            sents = [a + b + c for a, b, c in zip(sents, sents[1:], sents[2:])]\n",
    "        for sentence in sents[:num_sentences]:\n",
    "            tokenized = tokenizer(sentence, truncation=True, padding=\"max_length\", max_length=max_tokenized_length)[0]\n",
    "            valid_x_numerized.append(tokenized.ids)\n",
    "            valid_x_mask.append(tokenized.attention_mask)\n",
    "            valid_y_per_sentence.append(valid_y[i])\n",
    "\n",
    "    train_x_numerized = np.array(train_x_numerized)\n",
    "    train_x_mask = np.array(train_x_mask)\n",
    "    train_y_per_sentence = np.array(train_y_per_sentence)\n",
    "    valid_x_numerized = np.array(valid_x_numerized)\n",
    "    valid_x_mask = np.array(valid_x_mask)\n",
    "    valid_y_per_sentence = np.array(valid_y_per_sentence)\n",
    "    return train_x_numerized, train_x_mask, train_y_per_sentence, valid_x_numerized, valid_x_mask, valid_y_per_sentence\n",
    "\n",
    "from utils import memo_load\n",
    "finetuning_data_key = f\"cache-core/training-data-finetuning-max-tokens-{max_tokenized_length}-split-{split_hash}\"\n",
    "if sentence_pairs:\n",
    "    if sentence_pairs == \"3\":\n",
    "        finetuning_data_key += f\"-pairs\"\n",
    "    else:\n",
    "        finetuning_data_key += f\"-pairs-{sentence_pairs}\"\n",
    "(train_x_numerized, train_x_mask, train_y_per_sentence, valid_x_numerized, valid_x_mask, valid_y_per_sentence) = memo_load(\n",
    "    lambda: get_finetuning_data(),\n",
    "    finetuning_data_key\n",
    ")\n",
    "finetuning_data_hash = hash_file(finetuning_data_key + \".hkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-blues",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:55.747784Z",
     "iopub.status.busy": "2021-05-05T19:59:55.747408Z",
     "iopub.status.idle": "2021-05-05T19:59:55.763943Z",
     "shell.execute_reply": "2021-05-05T19:59:55.763683Z"
    },
    "papermill": {
     "duration": 0.028352,
     "end_time": "2021-05-05T19:59:55.764010",
     "exception": false,
     "start_time": "2021-05-05T19:59:55.735658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import ReviewPredictionModel\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_finetuning():\n",
    "    if regressive_style_finetuning:\n",
    "        embedding_bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
    "    else:\n",
    "        embedding_bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=9)\n",
    "\n",
    "    model_to_train_finetuning = ReviewPredictionModel(0, max_tokenized_length, regressive_bert_style=regressive_style_finetuning)\n",
    "    model_to_train_finetuning.transformer = embedding_bert\n",
    "    model_to_train_finetuning.to(device)\n",
    "    optimizer = optim.Adam(model_to_train_finetuning.parameters(), lr=lr_finetuning)\n",
    "    \n",
    "    training_accuracies_finetuning, validation_accuracies_finetuning = run_training_loop(\n",
    "        model_to_train_finetuning, optimizer, device,\n",
    "        batch_size_finetuning, epochs_finetuning,\n",
    "        train_x_numerized, train_x_mask, train_y_per_sentence, valid_x_numerized, valid_x_mask, valid_y_per_sentence,\n",
    "        max_validation_examples=256,\n",
    "        model_id=experiment_id, tag=\"finetuning\"\n",
    "    )\n",
    "    \n",
    "    return embedding_bert, training_accuracies_finetuning, validation_accuracies_finetuning\n",
    "\n",
    "def store_finetuning(tup, folder):\n",
    "    embedding_bert, training_accuracies_finetuning, validation_accuracies_finetuning = tup\n",
    "    th.save(embedding_bert.state_dict(), f\"{folder}/model.pt\")\n",
    "    hickle.dump((training_accuracies_finetuning, validation_accuracies_finetuning), f\"{folder}/accuracies.hkl\", mode=\"w\")\n",
    "\n",
    "def load_finetuning(folder):\n",
    "    if regressive_style_finetuning:\n",
    "        embedding_bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
    "    else:\n",
    "        embedding_bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=9)\n",
    "    embedding_bert.load_state_dict(th.load(f\"{folder}/model.pt\"))\n",
    "    embedding_bert.eval()\n",
    "    embedding_bert.to(device)\n",
    "    training_accuracies_finetuning, validation_accuracies_finetuning = hickle.load(f\"{folder}/accuracies.hkl\")\n",
    "    return embedding_bert, training_accuracies_finetuning, validation_accuracies_finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-edward",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:55.782201Z",
     "iopub.status.busy": "2021-05-05T19:59:55.781892Z",
     "iopub.status.idle": "2021-05-05T20:00:00.222800Z",
     "shell.execute_reply": "2021-05-05T20:00:00.222460Z"
    },
    "papermill": {
     "duration": 4.450865,
     "end_time": "2021-05-05T20:00:00.222881",
     "exception": false,
     "start_time": "2021-05-05T19:59:55.772016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from training_utils import run_training_loop\n",
    "\n",
    "from utils import memo_load\n",
    "finetuning_model_key = f\"cache-core/finetuning-batch-size-{batch_size_finetuning}-epochs-{epochs_finetuning}-lr-{lr_finetuning}-regressive-{regressive_style_finetuning}-data-{finetuning_data_hash}\"\n",
    "embedding_bert, training_accuracies_finetuning, validation_accuracies_finetuning = manual_memo(\n",
    "    train_finetuning, store_finetuning, load_finetuning,\n",
    "    finetuning_model_key\n",
    ")\n",
    "\n",
    "th.save(embedding_bert.state_dict(), f\"{experiment_dir}/finetuned-bert.pt\")\n",
    "hickle.dump((training_accuracies_finetuning, validation_accuracies_finetuning), f\"{experiment_dir}/finetuning-accuracies.hkl\", mode=\"w\")\n",
    "\n",
    "finetuning_model_hash = hash_file(finetuning_model_key + \"/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-accessory",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T20:00:00.259616Z",
     "iopub.status.busy": "2021-05-05T20:00:00.259010Z",
     "iopub.status.idle": "2021-05-05T20:00:00.625996Z",
     "shell.execute_reply": "2021-05-05T20:00:00.625414Z"
    },
    "papermill": {
     "duration": 0.387541,
     "end_time": "2021-05-05T20:00:00.626126",
     "exception": false,
     "start_time": "2021-05-05T20:00:00.238585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(training_accuracies_finetuning)), training_accuracies_finetuning, label = \"Training Accuracy\")\n",
    "plt.plot(list(map(lambda x: x * 100, range(len(validation_accuracies_finetuning)))), validation_accuracies_finetuning, label = \"Validation Accuracy\")\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-victorian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T20:00:00.669035Z",
     "iopub.status.busy": "2021-05-05T20:00:00.668033Z",
     "iopub.status.idle": "2021-05-05T20:00:09.817967Z",
     "shell.execute_reply": "2021-05-05T20:00:09.817417Z"
    },
    "papermill": {
     "duration": 9.173489,
     "end_time": "2021-05-05T20:00:09.818097",
     "exception": false,
     "start_time": "2021-05-05T20:00:00.644608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "embedding_bert.to(device)\n",
    "\n",
    "def get_embeds(x_data):\n",
    "    concatted_shape = None\n",
    "    pad_value = None\n",
    "    embeds = []\n",
    "    for text in tqdm(x_data):\n",
    "        doc = nlp(text)\n",
    "        embeddeds = []\n",
    "        sents = [str(sent) for sent in doc.sents]\n",
    "        if sentence_pairs == True and len(sents) > 1:\n",
    "            sents = [a + b for a, b in zip(sents, sents[1:])]\n",
    "        if sentence_pairs == \"3\" and len(sents) > 2:\n",
    "            sents = [a + b + c for a, b, c in zip(sents, sents[1:], sents[2:])]\n",
    "        all_tokenized = []\n",
    "        for sentence in sents[:num_sentences]:\n",
    "            tokenized = tokenizer(sentence, truncation=True, padding=\"max_length\", max_length=max_tokenized_length)[0]\n",
    "            all_tokenized.append(tokenized.ids)\n",
    "        \n",
    "        with th.no_grad():\n",
    "            sentence_tensor = th.tensor(all_tokenized).to(device)\n",
    "            concatted = np.concatenate([\n",
    "                # take output corresponding to CLS\n",
    "                embedding_bert.bert(sentence_tensor, output_hidden_states=True, return_dict=True)[1].cpu().numpy(),\n",
    "                np.zeros((len(all_tokenized), 1))\n",
    "            ], axis=1)\n",
    "            \n",
    "            if not concatted_shape:\n",
    "                concatted_shape = concatted.shape\n",
    "                pad_value = np.zeros(concatted_shape[1])\n",
    "                pad_value[-1] = 1\n",
    "            \n",
    "            embeddeds += list(concatted)\n",
    "\n",
    "        if len(sents) < num_sentences:\n",
    "            embeddeds += [pad_value] * (num_sentences - len(sents))\n",
    "\n",
    "        embeds.append(embeddeds)\n",
    "    return np.array(embeds)\n",
    "\n",
    "main_data_key = f\"cache-core/training-data-main-max-tokens-{max_tokenized_length}-split-{split_hash}-finetuned-{finetuning_model_hash}\"\n",
    "train_x_embeds, valid_x_embeds = memo_load(\n",
    "    lambda: (\n",
    "        get_embeds(train_x),\n",
    "        get_embeds(valid_x)\n",
    "    ),\n",
    "    main_data_key\n",
    ")\n",
    "main_data_hash = hash_file(main_data_key + \".hkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-delivery",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T20:00:09.859579Z",
     "iopub.status.busy": "2021-05-05T20:00:09.858988Z",
     "iopub.status.idle": "2021-05-05T20:00:09.876933Z",
     "shell.execute_reply": "2021-05-05T20:00:09.876370Z"
    },
    "papermill": {
     "duration": 0.040371,
     "end_time": "2021-05-05T20:00:09.877050",
     "exception": false,
     "start_time": "2021-05-05T20:00:09.836679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model_sentence_lstm import ReviewPredictionModel\n",
    "import torch as th\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_main():\n",
    "    model_to_train = ReviewPredictionModel(train_x_embeds.shape[2], lstm_hidden_size, bidi_lstm=lstm_bidi)\n",
    "    model_to_train.to(device)\n",
    "    optimizer = optim.Adam(model_to_train.parameters(), lr=main_model_lr)\n",
    "    \n",
    "    training_accuracies, validation_accuracies = run_training_loop(\n",
    "        model_to_train, optimizer, device,\n",
    "        batch_size, epochs,\n",
    "        train_x_embeds, None, np.array(train_y), valid_x_embeds, None, np.array(valid_y),\n",
    "        model_id=experiment_id, max_validation_examples=512\n",
    "    )\n",
    "    \n",
    "    return model_to_train, training_accuracies, validation_accuracies\n",
    "\n",
    "def store_main(tup, folder):\n",
    "    model_to_train, training_accuracies, validation_accuracies = tup\n",
    "    th.save(model_to_train.state_dict(), f\"{folder}/model.pt\")\n",
    "    hickle.dump((training_accuracies, validation_accuracies), f\"{folder}/accuracies.hkl\", mode=\"w\")\n",
    "\n",
    "def load_main(folder):\n",
    "    model_to_train = ReviewPredictionModel(train_x_embeds.shape[2], lstm_hidden_size, bidi_lstm=lstm_bidi)\n",
    "    model_to_train.load_state_dict(th.load(f\"{folder}/model.pt\"))\n",
    "    model_to_train.eval()\n",
    "    model_to_train.to(device)\n",
    "    training_accuracies, validation_accuracies = hickle.load(f\"{folder}/accuracies.hkl\")\n",
    "    return model_to_train, training_accuracies, validation_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-england",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T20:00:09.913827Z",
     "iopub.status.busy": "2021-05-05T20:00:09.913377Z",
     "iopub.status.idle": "2021-05-05T20:02:06.112338Z",
     "shell.execute_reply": "2021-05-05T20:02:06.111753Z"
    },
    "papermill": {
     "duration": 116.216854,
     "end_time": "2021-05-05T20:02:06.112464",
     "exception": false,
     "start_time": "2021-05-05T20:00:09.895610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from training_utils import run_training_loop\n",
    "\n",
    "bidi_key = \"-bidi\" if lstm_bidi else \"\"\n",
    "main_model_key = f\"cache-core/main-model-lstm-{lstm_hidden_size}{bidi_key}-lr-{main_model_lr}-batch-size-{batch_size}-epochs-{epochs}-data-{main_data_hash}\"\n",
    "main_model, training_accuracies, validation_accuracies = manual_memo(\n",
    "    train_main, store_main, load_main,\n",
    "    main_model_key\n",
    ")\n",
    "\n",
    "th.save(main_model.state_dict(), f\"{experiment_dir}/main-model.pt\")\n",
    "hickle.dump((training_accuracies, validation_accuracies), f\"{experiment_dir}/main-accuracies.hkl\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-fundamentals",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T20:02:12.337754Z",
     "iopub.status.busy": "2021-05-05T20:02:12.337453Z",
     "iopub.status.idle": "2021-05-05T20:02:12.498495Z",
     "shell.execute_reply": "2021-05-05T20:02:12.498747Z"
    },
    "papermill": {
     "duration": 3.304653,
     "end_time": "2021-05-05T20:02:12.498822",
     "exception": false,
     "start_time": "2021-05-05T20:02:09.194169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(training_accuracies)), training_accuracies, label = \"Training Accuracy\")\n",
    "plt.plot(list(map(lambda x: x * 100, range(len(validation_accuracies)))), validation_accuracies, label = \"Validation Accuracy\")\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 150.339669,
   "end_time": "2021-05-05T20:02:16.862478",
   "environment_variables": {},
   "exception": null,
   "input_path": "model-training-sentence-lstm.ipynb",
   "output_path": "model-training-sentence-lstm.ipynb",
   "parameters": {},
   "start_time": "2021-05-05T19:59:46.522809",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

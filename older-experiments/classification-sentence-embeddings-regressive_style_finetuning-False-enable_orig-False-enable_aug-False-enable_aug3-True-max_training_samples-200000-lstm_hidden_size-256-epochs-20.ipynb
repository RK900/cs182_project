{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "skilled-vertex",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T04:05:25.654253Z",
     "iopub.status.busy": "2021-05-08T04:05:25.653648Z",
     "iopub.status.idle": "2021-05-08T04:05:25.773048Z",
     "shell.execute_reply": "2021-05-08T04:05:25.772495Z"
    },
    "papermill": {
     "duration": 0.145452,
     "end_time": "2021-05-08T04:05:25.773172",
     "exception": false,
     "start_time": "2021-05-08T04:05:25.627720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from segtok import tokenizer\n",
    "from utils import *\n",
    "import hickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wrapped-diploma",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T04:05:25.808709Z",
     "iopub.status.busy": "2021-05-08T04:05:25.808118Z",
     "iopub.status.idle": "2021-05-08T04:05:25.816645Z",
     "shell.execute_reply": "2021-05-08T04:05:25.816119Z"
    },
    "papermill": {
     "duration": 0.027398,
     "end_time": "2021-05-08T04:05:25.816757",
     "exception": false,
     "start_time": "2021-05-08T04:05:25.789359",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Key Hyperparameters\n",
    "enable_orig = \"orig\"\n",
    "enable_aug = False\n",
    "enable_aug3 = False\n",
    "max_training_samples = 100000\n",
    "max_tokenized_length = 64\n",
    "num_sentences = 10\n",
    "valid_percent = 0.01\n",
    "\n",
    "batch_size_finetuning = 32\n",
    "epochs_finetuning = 1\n",
    "lr_finetuning = 1e-5\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "main_model_lr = 1e-5\n",
    "\n",
    "lstm_hidden_size = 1024\n",
    "regressive_style_finetuning = False\n",
    "\n",
    "experiment_id = f\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "qualified-facility",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T04:05:25.850664Z",
     "iopub.status.busy": "2021-05-08T04:05:25.850207Z",
     "iopub.status.idle": "2021-05-08T04:05:25.857015Z",
     "shell.execute_reply": "2021-05-08T04:05:25.856555Z"
    },
    "papermill": {
     "duration": 0.024371,
     "end_time": "2021-05-08T04:05:25.857103",
     "exception": false,
     "start_time": "2021-05-08T04:05:25.832732",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "regressive_style_finetuning = False\n",
    "enable_orig = False\n",
    "enable_aug = False\n",
    "enable_aug3 = True\n",
    "max_training_samples = 200000\n",
    "lstm_hidden_size = 256\n",
    "epochs = 20\n",
    "experiment_id = \"classification-sentence-embeddings-regressive_style_finetuning-False-enable_orig-False-enable_aug-False-enable_aug3-True-max_training_samples-200000-lstm_hidden_size-256-epochs-20\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rental-mechanism",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T04:05:25.886493Z",
     "iopub.status.busy": "2021-05-08T04:05:25.886040Z",
     "iopub.status.idle": "2021-05-08T04:05:25.901376Z",
     "shell.execute_reply": "2021-05-08T04:05:25.901856Z"
    },
    "papermill": {
     "duration": 0.032172,
     "end_time": "2021-05-08T04:05:25.901994",
     "exception": false,
     "start_time": "2021-05-08T04:05:25.869822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "experiment_dir = f\"completed-experiments/{experiment_id}\"\n",
    "if not os.path.exists(experiment_dir):\n",
    "    os.makedirs(experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sharing-prime",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T04:05:25.930995Z",
     "iopub.status.busy": "2021-05-08T04:05:25.930533Z",
     "iopub.status.idle": "2021-05-05T19:59:50.118615Z",
     "shell.execute_reply": "2021-05-05T19:59:50.117970Z"
    },
    "papermill": {
     "duration": 1.005971,
     "end_time": "2021-05-08T04:05:26.920792",
     "exception": false,
     "start_time": "2021-05-08T04:05:25.914821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_parsing import *\n",
    "data = load_dataset(\"./yelp_review_training_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-tampa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:50.154869Z",
     "iopub.status.busy": "2021-05-05T19:59:50.154250Z",
     "iopub.status.idle": "2021-05-05T19:59:53.089985Z",
     "shell.execute_reply": "2021-05-05T19:59:53.089435Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from training_utils import split_train_validation\n",
    "from text_preprocessing import preprocess\n",
    "import random\n",
    "\n",
    "def get_train_valid():\n",
    "    orig_train_x, valid_x, orig_train_y, valid_y = split_train_validation(data, 0.01)\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "\n",
    "    if enable_aug:\n",
    "        aug_data = load_gen_dataset(\"./new_data.json\") + load_gen_dataset(\"./new_data2.json\")\n",
    "        train_x += [i[0] for i in aug_data]\n",
    "        train_y += [i[1] for i in aug_data]\n",
    "\n",
    "    if enable_aug3:\n",
    "        aug_data3 = load_gen_dataset(\"./new_data3.json\")\n",
    "        train_x += [i[0] for i in aug_data3]\n",
    "        train_y += [i[1] for i in aug_data3]\n",
    "        \n",
    "    if enable_orig:\n",
    "        train_x += orig_train_x\n",
    "        train_y += orig_train_y\n",
    "    \n",
    "    train_x = train_x[:max_training_samples]\n",
    "    train_y = train_y[:max_training_samples]\n",
    "\n",
    "    if enable_orig == \"preprocess\":\n",
    "        train_x = preprocess(train_x)\n",
    "        valid_x = preprocess(valid_x)    \n",
    "\n",
    "    paired_train = list(zip(train_x, train_y))\n",
    "    random.shuffle(paired_train)\n",
    "    train_x = [i[0] for i in paired_train]\n",
    "    train_y = [i[1] for i in paired_train]\n",
    "\n",
    "    return [x.encode(\"utf-8\") for x in train_x], [x.encode(\"utf-8\") for x in valid_x], train_y, valid_y\n",
    "\n",
    "split_key = f\"cache-core/split-data-{valid_percent}-orig-{enable_orig}-aug12-{enable_aug}-aug3-{enable_aug3}-max-{max_training_samples}\"\n",
    "train_x, valid_x, train_y, valid_y = memo_load(\n",
    "    get_train_valid,\n",
    "    split_key\n",
    ")\n",
    "split_key = f\"cache-core/split-data-{valid_percent}-orig-{enable_orig}-aug12-{enable_aug}-aug3-{enable_aug3}-max-{len(train_x)}\"\n",
    "\n",
    "train_x = [x.decode(\"utf-8\") for x in train_x]\n",
    "valid_x = [x.decode(\"utf-8\") for x in valid_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-specific",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:53.122464Z",
     "iopub.status.busy": "2021-05-05T19:59:53.121867Z",
     "iopub.status.idle": "2021-05-05T19:59:53.133179Z",
     "shell.execute_reply": "2021-05-05T19:59:53.132587Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(train_x))\n",
    "print(len(train_y))\n",
    "print(len(valid_x))\n",
    "print(len(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-emission",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:53.164349Z",
     "iopub.status.busy": "2021-05-05T19:59:53.163871Z",
     "iopub.status.idle": "2021-05-05T19:59:54.053298Z",
     "shell.execute_reply": "2021-05-05T19:59:54.052694Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-silly",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:54.088081Z",
     "iopub.status.busy": "2021-05-05T19:59:54.087495Z",
     "iopub.status.idle": "2021-05-05T19:59:55.261425Z",
     "shell.execute_reply": "2021-05-05T19:59:55.260869Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-saver",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:55.305804Z",
     "iopub.status.busy": "2021-05-05T19:59:55.305209Z",
     "iopub.status.idle": "2021-05-05T19:59:55.727013Z",
     "shell.execute_reply": "2021-05-05T19:59:55.727279Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fine tune the BERT\n",
    "import numpy as np\n",
    "\n",
    "def get_finetuning_data():\n",
    "    train_x_numerized = []\n",
    "    train_x_mask = []\n",
    "    train_y_per_sentence = []\n",
    "    for i, text in tqdm(list(enumerate(train_x))):\n",
    "        doc = nlp(text)\n",
    "        sents = [str(sent) for sent in doc.sents]\n",
    "        for sentence in sents[:num_sentences]:\n",
    "            tokenized = tokenizer(sentence, truncation=True, padding=\"max_length\", max_length=max_tokenized_length)[0]\n",
    "            train_x_numerized.append(tokenized.ids)\n",
    "            train_x_mask.append(tokenized.attention_mask)\n",
    "            train_y_per_sentence.append(train_y[i])\n",
    "\n",
    "    valid_x_numerized = []\n",
    "    valid_x_mask = []\n",
    "    valid_y_per_sentence = []\n",
    "    for i, text in tqdm(list(enumerate(valid_x))):\n",
    "        doc = nlp(text)\n",
    "        sents = [str(sent) for sent in doc.sents]\n",
    "        for sentence in sents[:num_sentences]:\n",
    "            tokenized = tokenizer(sentence, truncation=True, padding=\"max_length\", max_length=max_tokenized_length)[0]\n",
    "            valid_x_numerized.append(tokenized.ids)\n",
    "            valid_x_mask.append(tokenized.attention_mask)\n",
    "            valid_y_per_sentence.append(valid_y[i])\n",
    "\n",
    "    train_x_numerized = np.array(train_x_numerized)\n",
    "    train_x_mask = np.array(train_x_mask)\n",
    "    train_y_per_sentence = np.array(train_y_per_sentence)\n",
    "    valid_x_numerized = np.array(valid_x_numerized)\n",
    "    valid_x_mask = np.array(valid_x_mask)\n",
    "    valid_y_per_sentence = np.array(valid_y_per_sentence)\n",
    "    return train_x_numerized, train_x_mask, train_y_per_sentence, valid_x_numerized, valid_x_mask, valid_y_per_sentence\n",
    "\n",
    "from utils import memo_load\n",
    "finetuning_data_key = f\"cache-core/training-data-finetuning-max-tokens-{max_tokenized_length}-split-{hash_key(split_key)}\"\n",
    "(train_x_numerized, train_x_mask, train_y_per_sentence, valid_x_numerized, valid_x_mask, valid_y_per_sentence) = memo_load(\n",
    "    lambda: get_finetuning_data(),\n",
    "    finetuning_data_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-blues",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:55.747784Z",
     "iopub.status.busy": "2021-05-05T19:59:55.747408Z",
     "iopub.status.idle": "2021-05-05T19:59:55.763943Z",
     "shell.execute_reply": "2021-05-05T19:59:55.763683Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import ReviewPredictionModel\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_finetuning():\n",
    "    embedding_bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=9)\n",
    "\n",
    "    model_to_train_finetuning = ReviewPredictionModel(0, max_tokenized_length, regressive_bert_style=regressive_style_finetuning)\n",
    "    model_to_train_finetuning.transformer = embedding_bert\n",
    "    model_to_train_finetuning.to(device)\n",
    "    optimizer = optim.Adam(model_to_train_finetuning.parameters(), lr=lr_finetuning)\n",
    "    \n",
    "    training_accuracies_finetuning, validation_accuracies_finetuning = run_training_loop(\n",
    "        model_to_train_finetuning, optimizer, device,\n",
    "        batch_size_finetuning, epochs_finetuning,\n",
    "        train_x_numerized, train_x_mask, train_y_per_sentence, valid_x_numerized, valid_x_mask, valid_y_per_sentence,\n",
    "        max_validation_examples=256,\n",
    "        model_id=experiment_id, tag=\"finetuning\"\n",
    "    )\n",
    "    \n",
    "    return embedding_bert, training_accuracies_finetuning, validation_accuracies_finetuning\n",
    "\n",
    "def store_finetuning(tup, folder):\n",
    "    embedding_bert, training_accuracies_finetuning, validation_accuracies_finetuning = tup\n",
    "    th.save(embedding_bert.state_dict(), f\"{folder}/model.pt\")\n",
    "    hickle.dump((training_accuracies_finetuning, validation_accuracies_finetuning), f\"{folder}/accuracies.hkl\", mode=\"w\")\n",
    "\n",
    "def load_finetuning(folder):\n",
    "    embedding_bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=9)\n",
    "    embedding_bert.load_state_dict(th.load(f\"{folder}/model.pt\"))\n",
    "    embedding_bert.eval()\n",
    "    embedding_bert.to(device)\n",
    "    training_accuracies_finetuning, validation_accuracies_finetuning = hickle.load(f\"{folder}/accuracies.hkl\")\n",
    "    return embedding_bert, training_accuracies_finetuning, validation_accuracies_finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-edward",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T19:59:55.782201Z",
     "iopub.status.busy": "2021-05-05T19:59:55.781892Z",
     "iopub.status.idle": "2021-05-05T20:00:00.222800Z",
     "shell.execute_reply": "2021-05-05T20:00:00.222460Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from training_utils import run_training_loop\n",
    "\n",
    "from utils import memo_load\n",
    "finetuning_model_key = f\"cache-core/finetuning-batch-size-{batch_size_finetuning}-epochs-{epochs_finetuning}-lr-{lr_finetuning}-regressive-{regressive_style_finetuning}-data-{hash_key(finetuning_data_key)}\"\n",
    "embedding_bert, training_accuracies_finetuning, validation_accuracies_finetuning = manual_memo(\n",
    "    train_finetuning, store_finetuning, load_finetuning,\n",
    "    finetuning_model_key\n",
    ")\n",
    "\n",
    "th.save(embedding_bert.state_dict(), f\"{experiment_dir}/finetuned-bert.pt\")\n",
    "hickle.dump((training_accuracies_finetuning, validation_accuracies_finetuning), f\"{experiment_dir}/finetuning-accuracies.hkl\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-accessory",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T20:00:00.259616Z",
     "iopub.status.busy": "2021-05-05T20:00:00.259010Z",
     "iopub.status.idle": "2021-05-05T20:00:00.625996Z",
     "shell.execute_reply": "2021-05-05T20:00:00.625414Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(training_accuracies_finetuning)), training_accuracies_finetuning, label = \"Training Accuracy\")\n",
    "plt.plot(list(map(lambda x: x * 100, range(len(validation_accuracies_finetuning)))), validation_accuracies_finetuning, label = \"Validation Accuracy\")\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-victorian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T20:00:00.669035Z",
     "iopub.status.busy": "2021-05-05T20:00:00.668033Z",
     "iopub.status.idle": "2021-05-05T20:00:09.817967Z",
     "shell.execute_reply": "2021-05-05T20:00:09.817417Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "embedding_bert.to(device)\n",
    "\n",
    "def get_embeds(x_data):\n",
    "    concatted_shape = None\n",
    "    pad_value = None\n",
    "    embeds = []\n",
    "    for text in tqdm(x_data):\n",
    "        doc = nlp(text)\n",
    "        embeddeds = []\n",
    "        sents = list(doc.sents)\n",
    "        all_tokenized = []\n",
    "        for sentence in sents[:num_sentences]:\n",
    "            sentence = str(sentence)\n",
    "            tokenized = tokenizer(sentence, truncation=True, padding=\"max_length\", max_length=max_tokenized_length)[0]\n",
    "            all_tokenized.append(tokenized.ids)\n",
    "        \n",
    "        with th.no_grad():\n",
    "            sentence_tensor = th.tensor(all_tokenized).to(device)\n",
    "            concatted = np.concatenate([\n",
    "                # take output corresponding to CLS\n",
    "                embedding_bert.bert(sentence_tensor, output_hidden_states=True, return_dict=True)[1].cpu().numpy(),\n",
    "                np.zeros((len(all_tokenized), 1))\n",
    "            ], axis=1)\n",
    "            \n",
    "            if not concatted_shape:\n",
    "                concatted_shape = concatted.shape\n",
    "                pad_value = np.zeros(concatted_shape[1])\n",
    "                pad_value[-1] = 1\n",
    "            \n",
    "            embeddeds += list(concatted)\n",
    "\n",
    "        if len(sents) < num_sentences:\n",
    "            embeddeds += [pad_value] * (num_sentences - len(sents))\n",
    "\n",
    "        embeds.append(embeddeds)\n",
    "    return np.array(embeds)\n",
    "\n",
    "main_data_key = f\"cache-core/training-data-main-max-tokens-{max_tokenized_length}-split-{hash_key(split_key)}-finetuned-{hash_key(finetuning_model_key)}\"\n",
    "train_x_embeds, valid_x_embeds = memo_load(\n",
    "    lambda: (\n",
    "        get_embeds(train_x),\n",
    "        get_embeds(valid_x)\n",
    "    ),\n",
    "    main_data_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-delivery",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T20:00:09.859579Z",
     "iopub.status.busy": "2021-05-05T20:00:09.858988Z",
     "iopub.status.idle": "2021-05-05T20:00:09.876933Z",
     "shell.execute_reply": "2021-05-05T20:00:09.876370Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model_sentence_lstm import ReviewPredictionModel\n",
    "import torch as th\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_main():\n",
    "    model_to_train = ReviewPredictionModel(train_x_embeds.shape[2], lstm_hidden_size)\n",
    "    model_to_train.to(device)\n",
    "    optimizer = optim.Adam(model_to_train.parameters(), lr=main_model_lr)\n",
    "    \n",
    "    training_accuracies, validation_accuracies = run_training_loop(\n",
    "        model_to_train, optimizer, device,\n",
    "        batch_size, epochs,\n",
    "        train_x_embeds, None, np.array(train_y), valid_x_embeds, None, np.array(valid_y),\n",
    "        model_id=experiment_id, max_validation_examples=512\n",
    "    )\n",
    "    \n",
    "    return model_to_train, training_accuracies, validation_accuracies\n",
    "\n",
    "def store_main(tup, folder):\n",
    "    model_to_train, training_accuracies, validation_accuracies = tup\n",
    "    th.save(model_to_train.state_dict(), f\"{folder}/model.pt\")\n",
    "    hickle.dump((training_accuracies, validation_accuracies), f\"{folder}/accuracies.hkl\", mode=\"w\")\n",
    "\n",
    "def load_main(folder):\n",
    "    model_to_train = ReviewPredictionModel(train_x_embeds.shape[2], lstm_hidden_size)\n",
    "    model_to_train.load_state_dict(th.load(f\"{folder}/model.pt\"))\n",
    "    model_to_train.eval()\n",
    "    model_to_train.to(device)\n",
    "    training_accuracies, validation_accuracies = hickle.load(f\"{folder}/accuracies.hkl\")\n",
    "    return model_to_train, training_accuracies, validation_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-england",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T20:00:09.913827Z",
     "iopub.status.busy": "2021-05-05T20:00:09.913377Z",
     "iopub.status.idle": "2021-05-05T20:02:06.112338Z",
     "shell.execute_reply": "2021-05-05T20:02:06.111753Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from training_utils import run_training_loop\n",
    "\n",
    "main_model_key = f\"cache-core/main-model-lstm-{lstm_hidden_size}-lr-{main_model_lr}-batch-size-{batch_size}-epochs-{epochs}-data-{hash_key(main_data_key)}\"\n",
    "main_model, training_accuracies, validation_accuracies = manual_memo(\n",
    "    train_main, store_main, load_main,\n",
    "    main_model_key\n",
    ")\n",
    "\n",
    "th.save(main_model.state_dict(), f\"{experiment_dir}/main-model.pt\")\n",
    "hickle.dump((training_accuracies, validation_accuracies), f\"{experiment_dir}/main-accuracies.hkl\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-fundamentals",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T20:02:12.337754Z",
     "iopub.status.busy": "2021-05-05T20:02:12.337453Z",
     "iopub.status.idle": "2021-05-05T20:02:12.498495Z",
     "shell.execute_reply": "2021-05-05T20:02:12.498747Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(training_accuracies)), training_accuracies, label = \"Training Accuracy\")\n",
    "plt.plot(list(map(lambda x: x * 100, range(len(validation_accuracies)))), validation_accuracies, label = \"Validation Accuracy\")\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "./model-training-sentence-lstm.ipynb",
   "output_path": "completed-experiments/classification-sentence-embeddings-regressive_style_finetuning-False-enable_orig-False-enable_aug-False-enable_aug3-True-max_training_samples-200000-lstm_hidden_size-256-epochs-20.ipynb",
   "parameters": {
    "enable_aug": false,
    "enable_aug3": true,
    "enable_orig": false,
    "epochs": 20,
    "experiment_id": "classification-sentence-embeddings-regressive_style_finetuning-False-enable_orig-False-enable_aug-False-enable_aug3-True-max_training_samples-200000-lstm_hidden_size-256-epochs-20",
    "lstm_hidden_size": 256,
    "max_training_samples": 200000,
    "regressive_style_finetuning": false
   },
   "start_time": "2021-05-08T04:05:24.646316",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "manufactured-chile",
   "metadata": {
    "papermill": {
     "duration": 0.034595,
     "end_time": "2021-05-08T22:19:23.706812",
     "exception": false,
     "start_time": "2021-05-08T22:19:23.672217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "necessary-commission",
   "metadata": {
    "papermill": {
     "duration": 0.021769,
     "end_time": "2021-05-08T22:19:23.749993",
     "exception": false,
     "start_time": "2021-05-08T22:19:23.728224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_folder = \".\"\n",
    "output_dir = os.path.join(root_folder, \"./model_save_gpt_234/\")\n",
    "data_path = os.path.join(root_folder, \"yelp_review_training_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "popular-trial",
   "metadata": {
    "papermill": {
     "duration": 0.014911,
     "end_time": "2021-05-08T22:19:23.775199",
     "exception": false,
     "start_time": "2021-05-08T22:19:23.760288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(root_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-average",
   "metadata": {
    "papermill": {
     "duration": 0.010405,
     "end_time": "2021-05-08T22:19:23.796096",
     "exception": false,
     "start_time": "2021-05-08T22:19:23.785691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Finetuned GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "threaded-packet",
   "metadata": {
    "papermill": {
     "duration": 0.57018,
     "end_time": "2021-05-08T22:19:24.376786",
     "exception": false,
     "start_time": "2021-05-08T22:19:23.806606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "collective-characterization",
   "metadata": {
    "papermill": {
     "duration": 2.985249,
     "end_time": "2021-05-08T22:19:27.372578",
     "exception": false,
     "start_time": "2021-05-08T22:19:24.387329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = GPT2Config.from_pretrained(output_dir)\n",
    "model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "asian-harvey",
   "metadata": {
    "papermill": {
     "duration": 1.918228,
     "end_time": "2021-05-08T22:19:29.301911",
     "exception": false,
     "start_time": "2021-05-08T22:19:27.383683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\"\n",
    "  model.cuda()\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-stupid",
   "metadata": {
    "papermill": {
     "duration": 0.022792,
     "end_time": "2021-05-08T22:19:29.348641",
     "exception": false,
     "start_time": "2021-05-08T22:19:29.325849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Do a sample review generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "brazilian-sessions",
   "metadata": {
    "papermill": {
     "duration": 0.014678,
     "end_time": "2021-05-08T22:19:29.378052",
     "exception": false,
     "start_time": "2021-05-08T22:19:29.363374",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoded_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e8b240074617>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoded_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "prompt = \"<|startoftext|> This place was\"\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "generated = encoded_inputs['input_ids']\n",
    "mask = encoded_inputs['attention_mask'].to(device)\n",
    "generated = generated.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "print(generated)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                generated, \n",
    "                                #bos_token_id=random.randint(1,30000),\n",
    "                                do_sample=True,   \n",
    "                                top_k=50, \n",
    "                                max_length = 300,\n",
    "                                top_p=0.95, \n",
    "                                num_return_sequences=2,\n",
    "                                attention_mask=mask\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chubby-press",
   "metadata": {
    "papermill": {
     "duration": 0.015235,
     "end_time": "2021-05-08T22:19:29.404299",
     "exception": false,
     "start_time": "2021-05-08T22:19:29.389064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_parsing import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mounted-whale",
   "metadata": {
    "papermill": {
     "duration": 2.567919,
     "end_time": "2021-05-08T22:19:31.983299",
     "exception": false,
     "start_time": "2021-05-08T22:19:29.415380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mechanical-graham",
   "metadata": {
    "papermill": {
     "duration": 0.102989,
     "end_time": "2021-05-08T22:19:32.104453",
     "exception": false,
     "start_time": "2021-05-08T22:19:32.001464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reviews234 = [i for i in data if i[1] == 2 or i[1] == 3 or i[1] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hidden-doctrine",
   "metadata": {
    "papermill": {
     "duration": 0.029163,
     "end_time": "2021-05-08T22:19:32.156301",
     "exception": false,
     "start_time": "2021-05-08T22:19:32.127138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142543"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "swedish-venice",
   "metadata": {
    "papermill": {
     "duration": 0.016968,
     "end_time": "2021-05-08T22:19:32.191491",
     "exception": false,
     "start_time": "2021-05-08T22:19:32.174523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join(root_folder, \"yelp_review_val_dataset_234.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bibliographic-update",
   "metadata": {
    "papermill": {
     "duration": 0.170791,
     "end_time": "2021-05-08T22:19:32.373706",
     "exception": false,
     "start_time": "2021-05-08T22:19:32.202915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text\n",
      "0      Visited this place for the second time when I ...\n",
      "1      We went here many times over the past few days...\n",
      "2      1.5 stars. This location has been a revolving ...\n",
      "3      After trying many of the dispensaries througho...\n",
      "4      Hard to figure out how to get a table, but the...\n",
      "...                                                  ...\n",
      "28504  I have only tried the italian beef. Outstandin...\n",
      "28505  I have had these pop ups since 2008 - they do ...\n",
      "28506  Came here for lunch and ordered fish. Service ...\n",
      "28507  Forces new patients to give a credit card # or...\n",
      "28508  First of all, they are supposed to open at 9:0...\n",
      "\n",
      "[28509 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_dir, error_bad_lines=False, sep='\\t', header='infer', lineterminator='\\n')  \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stock-arrest",
   "metadata": {
    "papermill": {
     "duration": 0.015556,
     "end_time": "2021-05-08T22:19:32.401160",
     "exception": false,
     "start_time": "2021-05-08T22:19:32.385604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels_dir = os.path.join(root_folder, \"yelp_review_val_dataset_234_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rising-signature",
   "metadata": {
    "papermill": {
     "duration": 0.022267,
     "end_time": "2021-05-08T22:19:32.435464",
     "exception": false,
     "start_time": "2021-05-08T22:19:32.413197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       text\n",
      "0       4.0\n",
      "1       3.0\n",
      "2       2.0\n",
      "3       3.0\n",
      "4       4.0\n",
      "...     ...\n",
      "28504   4.0\n",
      "28505   2.0\n",
      "28506   3.0\n",
      "28507   2.0\n",
      "28508   2.0\n",
      "\n",
      "[28509 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "labels_df = pd.read_csv(labels_dir, error_bad_lines=False, sep='\\t', header='infer', lineterminator='\\n')  \n",
    "print(labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "advanced-necklace",
   "metadata": {
    "papermill": {
     "duration": 0.021567,
     "end_time": "2021-05-08T22:19:32.469121",
     "exception": false,
     "start_time": "2021-05-08T22:19:32.447554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels234 = list(labels_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "educational-prescription",
   "metadata": {
    "papermill": {
     "duration": 0.02439,
     "end_time": "2021-05-08T22:19:32.505731",
     "exception": false,
     "start_time": "2021-05-08T22:19:32.481341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reviews234 = list(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "knowing-lancaster",
   "metadata": {
    "papermill": {
     "duration": 0.017945,
     "end_time": "2021-05-08T22:19:32.535859",
     "exception": false,
     "start_time": "2021-05-08T22:19:32.517914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28509, 28509)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews234), len(labels234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "compatible-sound",
   "metadata": {
    "papermill": {
     "duration": 0.017781,
     "end_time": "2021-05-08T22:19:32.566210",
     "exception": false,
     "start_time": "2021-05-08T22:19:32.548429",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Visited this place for the second time when I was in Montreal in August. I went for the first time in October after hearing marvelous things about the charcuterie (I was on a charcuterie kick at the time). My friend, Jason and I split the charcuterie plate with foie gras as well as a cheese plate (large portions--we had left overs). We both loved it! This time when I came, I got the charcuterie plate and a cronut (caramel). The charcuterie plate had changed a bit-- a couple items were replaced with new things. There was one \"deli\" style meat that I was not a fan of. Other than that I loved everything! There was a new ginger bread that they served with the foie grad that I was obsessed with. The combination was perfect! I also thoroughly enjoyed my cronut. They were a trend in the states for a bit, but sort of died out, so when I saw them on the menu, I knew I had to get one. It did not disappoint. The one negative thing I have to say is that the service did not seem to be super great/consistent. My service personally was fine, but there were people who came in after me and were made to wait before even being greeted by a hostess (they stood awkwardly in the doorway for 10 minutes with only 5-6 of the tables in the restaurant being occupied). The restaurant itself is cute and quaint and I\\'ll definitely be back for my charcuterie next time I\\'m in town!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews234[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "virgin-combining",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T22:19:32.594732Z",
     "iopub.status.busy": "2021-05-08T22:19:32.594453Z",
     "iopub.status.idle": "2021-05-08T22:19:32.596885Z",
     "shell.execute_reply": "2021-05-08T22:19:32.597123Z"
    },
    "papermill": {
     "duration": 0.018104,
     "end_time": "2021-05-08T22:19:32.597188",
     "exception": false,
     "start_time": "2021-05-08T22:19:32.579084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels234[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-attachment",
   "metadata": {
    "papermill": {
     "duration": 0.012949,
     "end_time": "2021-05-08T22:19:32.623180",
     "exception": false,
     "start_time": "2021-05-08T22:19:32.610231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create data for N number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "governmental-intelligence",
   "metadata": {
    "papermill": {
     "duration": 0.017682,
     "end_time": "2021-05-08T22:19:32.653934",
     "exception": false,
     "start_time": "2021-05-08T22:19:32.636252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_data = []\n",
    "num_to_generate = 2\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "antique-worst",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2021-05-08T22:19:32.667126",
     "status": "running"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3499 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 621, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 885, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 3/3499 [00:00<08:26,  6.91it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 4/3499 [00:01<17:08,  3.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 5/3499 [00:01<20:53,  2.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 6/3499 [00:01<19:54,  2.92it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 388, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 357, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 407, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 10/3499 [00:02<14:17,  4.07it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 11/3499 [00:03<17:14,  3.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 508, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 13/3499 [00:03<18:26,  3.15it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 972, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 450, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 393, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 17/3499 [00:04<16:49,  3.45it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 673, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 301, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 20/3499 [00:05<18:23,  3.15it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 315, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 417, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 437, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 489, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 1024, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 26/3499 [00:06<13:09,  4.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 444, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 28/3499 [00:08<20:15,  2.86it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 29/3499 [00:09<21:47,  2.65it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 30/3499 [00:09<24:23,  2.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 379, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 383, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 309, but ``max_length`` is set to 300.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 34/3499 [00:10<15:12,  3.80it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 34/3499 [00:10<17:31,  3.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c5de1a5dc2a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                   \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                   \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_to_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                   \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                                   )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/czd9rbay840drlh27357ds8sq84vhigx-python3.7-pytorch-1.6.0/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/courses/cs182/project/.venv/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, **model_kwargs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m                 \u001b[0moutput_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m             )\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/courses/cs182/project/.venv/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   1502\u001b[0m             \u001b[0;31m# pre-process distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mnext_token_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1504\u001b[0;31m             \u001b[0mnext_token_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_warper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m             \u001b[0;31m# Store scores, attentions and hidden_states when required\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/courses/cs182/project/.venv/lib/python3.7/site-packages/transformers/generation_logits_process.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/courses/cs182/project/.venv/lib/python3.7/site-packages/transformers/generation_logits_process.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0msorted_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0mcumulative_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, 28000 - batch_size, batch_size)):\n",
    "  text = reviews234[i % len(labels234) : (i + batch_size) % len(labels234)]\n",
    "  review = labels234[i % len(labels234) : (i + batch_size) % len(labels234)]\n",
    "#   prompt = \"<|startoftext|>\" + \" \" + ' '.join(text.split()[:10])\n",
    "  encodings = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "#   generated = torch.tensor([tokenizer.encode(\"<|startoftext|>\" + \" \" + ' '.join(p.split()[:7]), ) for p in text])\n",
    "#   generated = generated.to(device)\n",
    "  print(encodings.shape)\n",
    "  break\n",
    "  model = model.to(device)\n",
    "\n",
    "  sample_outputs = model.generate(\n",
    "                                  encodings['input_ids'], \n",
    "                                  #bos_token_id=random.randint(1,30000),\n",
    "                                  do_sample=True,   \n",
    "                                  top_k=50, \n",
    "                                  max_length = 300,\n",
    "                                  top_p=0.95, \n",
    "                                  num_return_sequences=num_to_generate,\n",
    "                                  attention_mask=encodings['attention_mask']\n",
    "                                  )\n",
    "    \n",
    "  assert len(sample_outputs) == batch_size * num_to_generate\n",
    "  for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "    gpt_output = tokenizer.decode(sample_output[0], skip_special_tokens=True).strip()\n",
    "    new_data.append((gpt_output, review[i % num_to_generate]))\n",
    "  \n",
    "#   gpt_output = tokenizer.decode(sample_outputs[0], skip_special_tokens=True).strip()\n",
    "#   new_data.append((gpt_output, review))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mineral-armstrong",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Yes', 4.0),\n",
       " ('Like walking back in time, every Saturday morning my sister and I was in a bowling league and after we were done, we\\'d spend a few quarters playing the pin ball machines until our mother came to pick us up.\\n\\nMy sister was daring and play the machines hard, she was afraid of that \"tilt\" showing up and freezing the game.  I, on the other hand was a bit more gentler and wanted to make sure I got my quarter\\'s worth.\\n\\nThis place has rows and rows of machines, some are really old and some are more of a mid 80\\'s theme.  There is even a Ms pac man!  It was fun to spend an afternoon playing the machines and remembering all the fun of my early teen years.',\n",
       "  4.0))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[10], data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-macedonia",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rs = json.dumps(dict(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-leave",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(root_folder, 'new_data_234_56000_samples.json'), 'w') as f:\n",
    "  json.dump(rs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-league",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "gpt2_data_aug.ipynb",
   "output_path": "gpt2_data_aug.ipynb",
   "parameters": {},
   "start_time": "2021-05-08T22:19:22.720120",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

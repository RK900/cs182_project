{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "rm -rf content\n",
    "ln -s /drive/MyDrive/cs182_project content\n",
    "cd content\n",
    "\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESTART YOUR RUNTIME NOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from segtok import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Hyperparameters\n",
    "max_tokenized_length = 64\n",
    "enable_orig = False\n",
    "enable_aug = False\n",
    "enable_aug3 = True\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 5\n",
    "\n",
    "experiment_id = \"classification-bert-64-tokens-5-epochs-aug3-only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "experiment_dir = f\"completed-experiments/{experiment_id}\"\n",
    "assert not os.path.exists(experiment_dir)\n",
    "os.makedirs(experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_parsing import load_dataset, load_gen_dataset\n",
    "data = load_dataset(\"./yelp_review_training_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_utils import split_train_validation\n",
    "train_x, valid_x, train_y, valid_y = split_train_validation(data, 0.01)\n",
    "\n",
    "if not enable_orig:\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "\n",
    "if enable_aug:\n",
    "    aug_data = load_gen_dataset(\"./new_data.json\") + load_gen_dataset(\"./new_data2.json\")\n",
    "    train_x += [i[0] for i in aug_data]\n",
    "    train_y += [i[1] for i in aug_data]\n",
    "\n",
    "if enable_aug3:\n",
    "    aug_data3 = load_gen_dataset(\"./new_data3.json\")\n",
    "    train_x += [i[0] for i in aug_data3]\n",
    "    train_y += [i[1] for i in aug_data3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_x))\n",
    "print(len(train_y))\n",
    "print(len(valid_x))\n",
    "print(len(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_numerized = []\n",
    "train_x_mask = []\n",
    "for text in tqdm(train_x):\n",
    "    tokenized = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_tokenized_length)[0]\n",
    "    train_x_numerized.append(tokenized.ids)\n",
    "    train_x_mask.append(tokenized.attention_mask)\n",
    "valid_x_numerized = []\n",
    "valid_x_mask = []\n",
    "for text in tqdm(valid_x):\n",
    "    tokenized = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_tokenized_length)[0]\n",
    "    valid_x_numerized.append(tokenized.ids)\n",
    "    valid_x_mask.append(tokenized.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_x_numerized = np.array(train_x_numerized)\n",
    "train_x_mask = np.array(train_x_mask)\n",
    "train_y = np.array(train_y)\n",
    "valid_x_numerized = np.array(valid_x_numerized)\n",
    "valid_x_mask = np.array(valid_x_mask)\n",
    "valid_y = np.array(valid_y)\n",
    "\n",
    "from utils import memo_load\n",
    "(train_x_numerized, train_x_mask, train_y, valid_x_numerized, valid_x_mask, valid_y) = memo_load(\n",
    "    lambda: (train_x_numerized, train_x_mask, train_y, valid_x_numerized, valid_x_mask, valid_y),\n",
    "    f\"{experiment_dir}/training_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ReviewPredictionModel\n",
    "import torch as th\n",
    "import torch.optim as optim\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model_to_train = None\n",
    "model_to_train = ReviewPredictionModel(0, max_tokenized_length)\n",
    "model_to_train.to(device)\n",
    "optimizer = optim.Adam(model_to_train.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_utils import run_training_loop\n",
    "\n",
    "training_accuracies, validation_accuracies = run_training_loop(\n",
    "    model_to_train, optimizer, device,\n",
    "    batch_size, epochs,\n",
    "    train_x_numerized, train_x_mask, train_y, valid_x_numerized, valid_x_mask, valid_y,\n",
    "    model_id=experiment_id\n",
    ")\n",
    "\n",
    "from utils import memo_load\n",
    "(training_accuracies, validation_accuracies) = memo_load(\n",
    "    lambda: (training_accuracies, validation_accuracies),\n",
    "    f\"{experiment_dir}/training_validation_accuracies\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(training_accuracies)), training_accuracies, label = \"Training Accuracy\")\n",
    "plt.plot(list(map(lambda x: x * 100, range(len(validation_accuracies)))), validation_accuracies, label = \"Validation Accuracy\")\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{experiment_dir}/training-plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "rm -rf content\n",
    "ln -s /drive/MyDrive/cs182_project content\n",
    "cd content\n",
    "\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESTART YOUR RUNTIME NOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from segtok import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Hyperparameters\n",
    "max_tokenized_length = 64\n",
    "enable_orig = False\n",
    "enable_aug = False\n",
    "enable_aug3 = True\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 5\n",
    "\n",
    "experiment_id = \"classification-bert-64-tokens-5-epochs-aug3-only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "experiment_dir = f\"completed-experiments/{experiment_id}\"\n",
    "assert not os.path.exists(experiment_dir)\n",
    "os.makedirs(experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_parsing import load_dataset, load_gen_dataset\n",
    "data = load_dataset(\"./yelp_review_training_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_utils import split_train_validation\n",
    "train_x, valid_x, train_y, valid_y = split_train_validation(data, 0.01)\n",
    "\n",
    "if not enable_orig:\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "\n",
    "if enable_aug:\n",
    "    aug_data = load_gen_dataset(\"./new_data.json\") + load_gen_dataset(\"./new_data2.json\")\n",
    "    train_x += [i[0] for i in aug_data]\n",
    "    train_y += [i[1] for i in aug_data]\n",
    "\n",
    "if enable_aug3:\n",
    "    aug_data3 = load_gen_dataset(\"./new_data3.json\")\n",
    "    train_x += [i[0] for i in aug_data3]\n",
    "    train_y += [i[1] for i in aug_data3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_x))\n",
    "print(len(train_y))\n",
    "print(len(valid_x))\n",
    "print(len(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_numerized = []\n",
    "train_x_mask = []\n",
    "for text in tqdm(train_x):\n",
    "    tokenized = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_tokenized_length)[0]\n",
    "    train_x_numerized.append(tokenized.ids)\n",
    "    train_x_mask.append(tokenized.attention_mask)\n",
    "valid_x_numerized = []\n",
    "valid_x_mask = []\n",
    "for text in tqdm(valid_x):\n",
    "    tokenized = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_tokenized_length)[0]\n",
    "    valid_x_numerized.append(tokenized.ids)\n",
    "    valid_x_mask.append(tokenized.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_x_numerized = np.array(train_x_numerized)\n",
    "train_x_mask = np.array(train_x_mask)\n",
    "train_y = np.array(train_y)\n",
    "valid_x_numerized = np.array(valid_x_numerized)\n",
    "valid_x_mask = np.array(valid_x_mask)\n",
    "valid_y = np.array(valid_y)\n",
    "\n",
    "from utils import memo_load\n",
    "(train_x_numerized, train_x_mask, train_y, valid_x_numerized, valid_x_mask, valid_y) = memo_load(\n",
    "    lambda: (train_x_numerized, train_x_mask, train_y, valid_x_numerized, valid_x_mask, valid_y),\n",
    "    f\"{experiment_dir}/training_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ReviewPredictionModel\n",
    "import torch as th\n",
    "import torch.optim as optim\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model_to_train = None\n",
    "model_to_train = ReviewPredictionModel(0, max_tokenized_length)\n",
    "model_to_train.to(device)\n",
    "optimizer = optim.Adam(model_to_train.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_utils import run_training_loop\n",
    "\n",
    "training_accuracies, validation_accuracies = run_training_loop(\n",
    "    model_to_train, optimizer, device,\n",
    "    batch_size, epochs,\n",
    "    train_x_numerized, train_x_mask, train_y, valid_x_numerized, valid_x_mask, valid_y,\n",
    "    model_id=experiment_id\n",
    ")\n",
    "\n",
    "from utils import memo_load\n",
    "(training_accuracies, validation_accuracies) = memo_load(\n",
    "    lambda: (training_accuracies, validation_accuracies),\n",
    "    f\"{experiment_dir}/training_validation_accuracies\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(training_accuracies)), training_accuracies, label = \"Training Accuracy\")\n",
    "plt.plot(list(map(lambda x: x * 100, range(len(validation_accuracies)))), validation_accuracies, label = \"Validation Accuracy\")\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{experiment_dir}/training-plot.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
